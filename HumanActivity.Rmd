###Classifying quality of exercise using monitor 

by Fatima Mouaki
3/22/2015

####sypnosis

Given both training and test data from the following study:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

the goal of this project is to "predict the manner in which they did the exercise."

Further, Professor Leek states that this report should describe:

"how you built your model"
"how you used cross validation"
"what you think the expected out of sample error is"
"why you made the choices you did"
Ultimately, the prediction model is to be run on the test data to predict the outcome of 20 different test cases.

##### Loading Data

```{r, echo=TRUE}
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)

```
training<-read.csv("pml-training.csv")




Read data and replace missing values with NA

```{r, echo=TRUE}
training<-read.csv("pml-training.csv",na.strings=c("NA",""),header=TRUE)
testing<-read.csv("pml-testing.csv",na.strings=c("NA",""),header=TRUE)

```

clean and process data

```{r, echo=TRUE}

# Count the number of non-NAs in each col.
nonNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}
# Build vector of NA columns to remove.

colnames_train<-colnames(training)
colnames_test<-colnames(testing)

coln<-nonNAs(training)

NA_remove<-c()
for (i in 1:length(coln)) {
    if (coln[i] < nrow(training)) {
        NA_remove <- c(NA_remove, colnames_train[i])
    }
}

df_training <- training[,!(names(training) %in% NA_remove)]
df_testing<-testing[,!(names(testing) %in% NA_remove)]

#The 7 first columns are unecessary, lets remove them from training data.

df_training <- df_training[,8:length(colnames(df_training))]
df_testing <- df_testing[,8:length(colnames(df_testing))]


colnames(df_training)
colnames(df_testing)

```



Let's identify varaiables that have no variability.

nsv <- nearZeroVar(df_training, saveMetrics=TRUE)

```{r, echo=TRUE}

nsv <- nearZeroVar(df_training, saveMetrics=TRUE)
nsv
```


Given that all of the near zero variance variables (nsv) are FALSE, there's no need to eliminate any covariates due to lack of variablility.

#####Partitioning the training data set

The training data set contains 53 variables and 19622 obs. The testing data set contains 53 variables and 20 obs. In order to perform cross-validation, the training data set is partionned into 2 sets: subTraining (75%) and subTest (25%). This will be performed using random subsampling without replacement.

```{r, echo=TRUE}
library(AppliedPredictiveModeling)
library(caret)
library(rattle)

subsamples<-createDataPartition(y=df_training$classe, p=0.75, list=FALSE)
subTraining <- df_training[subsamples, ]
subTesting<-df_training[-subsamples, ]

dim(subTraining)
dim(subTesting)

```

##### Exploratory analysis

```{r,echo=TRUE}

plot(subTraining$classe, col="blue", main="levels of the variable classe within the subTraining data set", xlab="classe levels", ylab="Frequency")
```

The variable "classe" contains 5 levels: A, B, C, D and E.  Level A is the most frequent with a little more than 4000 occurrences while level D is the least frequent with about 2500 occurrences.

##### EVALUATION


###### Classification Tree

```{r,echo=TRUE}

modFit <- train(classe~ ., data = subTraining, method="rpart")

print(modFit$finalModel, digits=3)
fancyRpartPlot(modFit$finalModel)
```

Run against testing set.

```{r,echo=TRUE}
# Run against testing set 1 of 4 with no extra features.
library(caret)
prediction <- predict(modFit, subTesting)
confusionMatrix(prediction, subTesting$classe)
```

Model accuarcy is near 5%, which is diappointing. Let's use preprocessing method.

```{r,echo=TRUE}

modFit <- train(classe~ ., data = subTraining, preProcess=c("center", "scale"), method="rpart")
prediction <- predict(modFit, subTesting)
confusionMatrix(prediction, subTesting$classe)
```

Model accuracy is always near 5%. Let's use RandomForest, this should be a better model. 

###### Random Forest

```{r,echo=TRUE}
library(caret)
library(randomForest)

modFit1<-randomForest(classe ~. , data=subTraining, method="class")
#modFit <- train(classe ~ ., data=subTraining, method="rf", prox=TRUE)

#print(modFit$finalModel, digits=3)
prediction<- predict(modFit1, subTesting, type = "class")
confusionMatrix(prediction, subTesting$classe)
```

As Expected, Random Forest algorithm performed better than Decision Trees. Accuracy for RandomForest model was 99%. The expected out-of-sample error is estimated at 1-accuracy, which is 0.5%. I will be choosing this model for submission.


###### Submission

Predict outcome levels on the original Testing data set using Random Forest algorithm

```{r,echo=TRUE}

predictfinal <- predict(modFit1, df_testing, type="class")
predictfinal
```


```{r,echo=TRUE}
#writing the output
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictfinal)
```
